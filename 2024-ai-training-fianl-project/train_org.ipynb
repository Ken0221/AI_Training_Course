{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J93a2JElfqlj",
    "outputId": "73e52d48-6a39-436e-a3f7-b5d108b12027"
   },
   "outputs": [],
   "source": [
    "# !pip install thop\n",
    "# !pip install torchsummary\n",
    "# !pip install einops\n",
    "# !pip install -q kaggle\n",
    "# !pip install torch\n",
    "# !pip install numpy\n",
    "# !pip install opencv-python\n",
    "# !pip install matplotlib\n",
    "# !pip install natsort\n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "uJUnHoY3ag_1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from glob import glob\n",
    "from natsort import natsorted\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from thop import profile\n",
    "from thop import clever_format\n",
    "from torchsummary import summary\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.facecolor'] = '#ffffff'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: NVIDIA GeForce MX250\n"
     ]
    }
   ],
   "source": [
    "result = torch.cuda.get_device_name(torch.cuda.current_device()) if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Current device: \" + result)\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDCdGBRHhNwR"
   },
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x4sCPVAkZjYp",
    "outputId": "62a1a022-ed50-4212-a3ae-cf4e9e8e1a77"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'unzip' ���O�����Υ~���R�O�B�i���檺�{���Χ妸�ɡC\n"
     ]
    }
   ],
   "source": [
    "!unzip Set5.zip\n",
    "# !rm ./adarlab-ai-training.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gBEhrxWhRSR"
   },
   "source": [
    "# Checking Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "YRR5rwFsfwl4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pwd' ���O�����Υ~���R�O�B�i���檺�{���Χ妸�ɡC\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "img_data_dir = './Set5/Set5/LRbicx3'\n",
    "images = natsorted(glob(os.path.join(img_data_dir, '*.png')))\n",
    "print(len(images))\n",
    "\n",
    "ans_data_dir = './Set5/Set5/original'\n",
    "ans = natsorted(glob(os.path.join(ans_data_dir, '*.png')))\n",
    "print(len(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3462RLc7cxvd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9P5TGP47hDPR"
   },
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "9BDTfi-5fp5q"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"data_dir\": './Set5',\n",
    "  \"data_num\": 5,\n",
    "  \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "  # you can set your own training configurations\n",
    "  \"batch_size\": 1,\n",
    "  \"learning_rate\": 0.0005,\n",
    "  \"n_epochs\": 3,\n",
    "  \"rot_num\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = None\n",
    "# transform = transforms.Compose{\n",
    "    \n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "6RRZpsGTfejS"
   },
   "outputs": [],
   "source": [
    "class TrainImg(Dataset):\n",
    "  def __init__(self, config, set_type=\"train\", transform=None):\n",
    "    self.device = config[\"device\"]\n",
    "    self.transform = transform\n",
    "    ###########################your code here#############################\n",
    "    # get the image path list -> self.image_names\n",
    "    self.image_names = images\n",
    "    self.ans_names = ans\n",
    "\n",
    "    ########################################################################\n",
    "    if set_type == \"train\":\n",
    "        n_start = 0\n",
    "        n_end = 4\n",
    "    elif set_type == \"val\":\n",
    "        n_start = 4\n",
    "        n_end = config['data_num']\n",
    "\n",
    "    self.image_names = self.image_names[n_start:n_end]\n",
    "    self.ans_names = self.ans_names[n_start:n_end]\n",
    "  \n",
    "    ########################################################################\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.image_names)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    ########################################################################\n",
    "    # get the input image step by step\n",
    "    # 1. read the image using cv2\n",
    "    # 2. transpose the dimension from [h, w, 3] to [3, h, w]\n",
    "    # 3. from numpy array to tensor\n",
    "    # 4. normalize the value from [0, 255] to [0, 1]\n",
    "    image_name = self.image_names[idx]\n",
    "    image = cv2.imread(image_name, cv2.IMREAD_COLOR)\n",
    "    image = np.transpose(image, (2, 0, 1))  # transpose the dimension from [h, w, 3] to [3, h, w]\n",
    "    image = torch.from_numpy(image).float() # from numpy array to tensor\n",
    "    # image = image / 255 # normalize the value from [0, 255] to [0, 1]\n",
    "    \n",
    "    image_rot_arr = []\n",
    "    image_rot_arr.append(image)\n",
    "    length = config[\"rot_num\"] - 1\n",
    "    if self.transform:\n",
    "        for i in range(length):\n",
    "            image_rot = self.transform((image))\n",
    "            image_rot_arr.append(image_rot)\n",
    "    else:\n",
    "        for i in range(length):\n",
    "            image_rot_arr.append(image)\n",
    "    \n",
    "    image = image / 255 # normalize the value from [0, 255] to [0, 1]\n",
    "    \n",
    "    for i in range(length + 1):\n",
    "        image_rot_arr[i] = image_rot_arr[i] / 255\n",
    "        \n",
    "    ans_name = self.ans__names[idx]\n",
    "    ans = cv2.imread(ans_name, cv2.IMREAD_COLOR)\n",
    "    ans = np.transpose(ans, (2, 0, 1))  # transpose the dimension from [h, w, 3] to [3, h, w]\n",
    "    ans = torch.from_numpy(ans).float() # from numpy array to tensor\n",
    "    # image = image / 255 # normalize the value from [0, 255] to [0, 1]\n",
    "    \n",
    "    ans_rot_arr = []\n",
    "    ans_rot_arr.append(ans)\n",
    "    length = config[\"rot_num\"] - 1\n",
    "    if self.transform:\n",
    "        for i in range(length):\n",
    "            ans_rot = self.transform((ans))\n",
    "            ans_rot_arr.append(ans_rot)\n",
    "    else:\n",
    "        for i in range(length):\n",
    "            ans_rot_arr.append(ans)\n",
    "    \n",
    "    ans = ans / 255 # normalize the value from [0, 255] to [0, 1]\n",
    "    \n",
    "    for i in range(length + 1):\n",
    "        ans_rot_arr[i] = ans_rot_arr[i] / 255\n",
    "    \n",
    "    return {\n",
    "        'image_name': image_name,\n",
    "        'image': image_rot_arr,\n",
    "        'ans': ans_rot_arr,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "ncyC-Hrc8wx1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset length:  5\n",
      "Train dataset length:  4\n",
      "Validation dataset length:  1\n"
     ]
    }
   ],
   "source": [
    "train_ds = TrainImg(config, set_type='train', transform=transform)\n",
    "val_ds = TrainImg(config, set_type='val')\n",
    "\n",
    "train_dl = DataLoader(train_ds, config[\"batch_size\"], shuffle=True, drop_last=True, num_workers=1)\n",
    "val_dl = DataLoader(val_ds, config[\"batch_size\"], shuffle=True, drop_last=True, num_workers=1)\n",
    "\n",
    "print(\"Total dataset length: \", config['data_num'])\n",
    "print(\"Train dataset length: \", len(train_ds))\n",
    "print(\"Validation dataset length: \", len(val_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKHgOCW7hAYS"
   },
   "source": [
    "# Show images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_img(image, vis=None, color_fixed=None, linewidth=1, img_order='rgb', draw_kp=True, kp_style=None):\n",
    "  \"\"\" Inpaints a hand stick figure into a matplotlib figure.\n",
    "  image:    original image input\n",
    "  coords_hw:  predicted keypoint (non normalized) -> [0, 224)\n",
    "  \"\"\"\n",
    "  if kp_style is None:\n",
    "    # kp_style[0] for circle radius, kp_style[1] for circle point thickness\n",
    "    kp_style = (1, 2)\n",
    "\n",
    "  # if image have four dimension like [1. 224. 224. 3] then squeeze to [3. 224. 3]\n",
    "  image = np.squeeze(image)\n",
    "\n",
    "  if len(image.shape) == 2:\n",
    "    image = np.expand_dims(image, 2)\n",
    "  s = image.shape\n",
    "  assert len(s) == 3, \"This only works for single images.\"\n",
    "\n",
    "  convert_to_uint8 = False\n",
    "\n",
    "  if s[2] == 1:\n",
    "    # grayscale case\n",
    "    image = (image - np.min(image)) / (np.max(image) - np.min(image) + 1e-4)\n",
    "    image = np.tile(image, [1, 1, 3])\n",
    "    pass\n",
    "\n",
    "  elif s[2] == 3:\n",
    "    # RGB case\n",
    "    if image.dtype == np.uint8:\n",
    "        convert_to_uint8 = True\n",
    "        image = image.astype('float64') / 255.0\n",
    "    elif image.dtype == np.float32:\n",
    "        # convert to gray image\n",
    "        image = np.mean(image, axis=2)\n",
    "        image = (image - np.min(image)) / (np.max(image) - np.min(image) + 1e-4)\n",
    "        image = np.expand_dims(image, 2)\n",
    "        image = np.tile(image, [1, 1, 3])\n",
    "  else:\n",
    "    assert 0, \"Unknown image dimensions.\"\n",
    "\n",
    "  colors = np.array(\n",
    "    [[0.4, 0.4, 0.4],\n",
    "    [0.4, 0.0, 0.0],\n",
    "    [0.6, 0.0, 0.0],\n",
    "    [0.8, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [0.4, 0.4, 0.0],\n",
    "    [0.6, 0.6, 0.0],\n",
    "    [0.8, 0.8, 0.0],\n",
    "    [1.0, 1.0, 0.0],\n",
    "    [0.0, 0.4, 0.2],\n",
    "    [0.0, 0.6, 0.3],\n",
    "    [0.0, 0.8, 0.4],\n",
    "    [0.0, 1.0, 0.5],\n",
    "    [0.0, 0.2, 0.4],\n",
    "    [0.0, 0.3, 0.6],\n",
    "    [0.0, 0.4, 0.8],\n",
    "    [0.0, 0.5, 1.0],\n",
    "    [0.4, 0.0, 0.4],\n",
    "    [0.6, 0.0, 0.6],\n",
    "    [0.7, 0.0, 0.8],\n",
    "    [1.0, 0.0, 1.0]]\n",
    "  )\n",
    "\n",
    "  if img_order == 'rgb':\n",
    "    # cv2 operation under BGR\n",
    "    colors = colors[:, ::-1]\n",
    "\n",
    "  color_map = {\n",
    "    'k': np.array([0.0, 0.0, 0.0]),\n",
    "    'w': np.array([1.0, 1.0, 1.0]),\n",
    "    'b': np.array([0.0, 0.0, 1.0]),\n",
    "    'g': np.array([0.0, 1.0, 0.0]),\n",
    "    'r': np.array([1.0, 0.0, 0.0]),\n",
    "    'm': np.array([1.0, 1.0, 0.0]),\n",
    "    'c': np.array([0.0, 1.0, 1.0])\n",
    "  }\n",
    "\n",
    "  if convert_to_uint8:\n",
    "    image = (image * 255).astype('uint8')\n",
    "\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 5108) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Ken\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1131\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1131\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mc:\\Users\\Ken\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m batch_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(train_dl)\n\u001b[1;32m----> 2\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m imgs \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;66;03m#return[img1, img2, img3]\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# print(batch['image'])\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ken\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Ken\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1327\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ken\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1293\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1289\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1290\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1293\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1294\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1295\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\Ken\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1144\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1143\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 5108) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "batch_iter = iter(train_dl)\n",
    "batch = next(batch_iter)\n",
    "\n",
    "imgs = batch['image'] #return[img1, img2, img3]\n",
    "# print(batch['image'])\n",
    "pic_num = 4\n",
    "fig, axes = plt.subplots(len(imgs), pic_num, figsize=(15, 10))\n",
    "for i in range(len(imgs)):\n",
    "    for j in range(pic_num):\n",
    "        # 將圖像和關鍵點轉換為 NumPy 格式\n",
    "        img_np = imgs[i][j].permute(1, 2, 0).numpy()\n",
    "\n",
    "        # 使用 draw_hand 函數繪製關鍵點\n",
    "        trainimg = draw_img\n",
    "        \n",
    "        # 顯示結果\n",
    "        axes[i, j].imshow(trainimg)\n",
    "        # axes[i, j].axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eb_pWxc2xmOM",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Model\n",
    "\n",
    "+ Model Specifications:\n",
    "  + Input: **`[B, 3, 224, 224]`**\n",
    "  + Output: **`[B, 21, 2]`** --> 21 for the num of the landmarks, 2 for the coordinates in (x, y) format\n",
    "  + Layer: You can build up your own model architecture with no limitations.\n",
    "  + Cost: The computational cost (FLOPs) may not over **`20 GFLOPs`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "w9mqC8nZW7Tg"
   },
   "outputs": [],
   "source": [
    "from model import SuperResolution\n",
    "\n",
    "Net = SuperResolution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5cExNrO9Ad6",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Testing Model Computational Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "yTf0tYV2eFSh"
   },
   "outputs": [],
   "source": [
    "# # pseudo image\n",
    "# image = torch.rand(1, 3, 224, 224).cuda()\n",
    "\n",
    "# # define your model\n",
    "model = Net.to(config[\"device\"])\n",
    "\n",
    "# out = model(image)\n",
    "\n",
    "# # torchsummary report\n",
    "# summary(model, input_size=(3, 224, 224))\n",
    "# print(f'From input shape: {image.shape} to output shape: {out.shape}')\n",
    "\n",
    "# # thop report\n",
    "# macs, parm = profile(model, inputs=(image, ))\n",
    "# print(f'FLOPS: {macs * 2 / 1e9} G, Params: {parm / 1e6} M.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BB7B2iQI8IQb"
   },
   "source": [
    "# Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "m0KvUVwb8NjQ"
   },
   "outputs": [],
   "source": [
    "# class CMSELoss(nn.Module):\n",
    "#   \"\"\"\n",
    "#   Coordinate MSE loss\n",
    "#   input :\n",
    "#     y_pred = b, 21, 2   (coordinate of 21 keypoints)\n",
    "#     y_true = b, 21, 2   (keypoints, (y, x))\n",
    "#   \"\"\"\n",
    "#   def __init__(self):\n",
    "#     super().__init__()\n",
    "#     self.loss = nn.MSELoss()\n",
    "\n",
    "#   def forward(self, y_pred, y_true):\n",
    "#     y_true = torch.flip(y_true, [2]) # flip (y, x) to (x, y)\n",
    "#     return self.loss(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "AmZhlbbPhKJU"
   },
   "outputs": [],
   "source": [
    "# criterion = CMSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xGp8--mK9o8U"
   },
   "source": [
    "# Optimizer and Scheduler (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "rHIG6WpB9pyL"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "# optimizer = torch.optim.Adadelta(model.parameters(), lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "0HGT7xWcO59p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0\n"
     ]
    }
   ],
   "source": [
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.002, steps_per_epoch=len(train_dl), epochs=config[\"n_epochs\"])\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, steps_per_epoch=len(train_dl), epochs=config[\"n_epochs\"], div_factor=2, final_div_factor=5, pct_start=0.09)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = config[\"rot_num\"] * (len(train_dl.dataset) / config[\"batch_size\"]) * config[\"n_epochs\"], eta_min = 0.00005)\n",
    "print((len(train_dl.dataset) / config[\"batch_size\"]) * config[\"n_epochs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvispd49hh0p"
   },
   "source": [
    "# Training\n",
    "  + Record the **`loss / epoch`** learning curve\n",
    "  + If using learning rate scheduler, record the **`lr / epoch`** curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_path = '/home/va8800/ken_ai/model_weights.pth'\n",
    "# weight_path = '/home/STuser19/MID/model_weights.pth'\n",
    "\n",
    "# checkpoint = torch.load(weight_path)\n",
    "# model.load_state_dict(checkpoint, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "CxgfsfnDeUft"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:15<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 1876) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Ken\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1131\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1131\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mc:\\Users\\Ken\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 21\u001b[0m\n\u001b[0;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#######################\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# train the model #\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#######################\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dl):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m])):\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;66;03m# clear the gradients of all optimized variables\u001b[39;00m\n\u001b[0;32m     24\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\Ken\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ken\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Ken\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1327\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ken\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1293\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1289\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1290\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1293\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1294\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1295\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\Ken\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1144\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1143\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 1876) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "# initialize tracker for minimum validation loss\n",
    "# valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "valid_loss_min = np.inf # set initial \"min\" to infinity\n",
    "\n",
    "# initialize history for recording what we want to know\n",
    "history = []\n",
    "\n",
    "for epoch in range(config[\"n_epochs\"]):\n",
    "    # monitor training loss, validation loss and learning rate\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    lrs    = []\n",
    "    result = {'train_loss': [], 'val_loss': [], 'lrs': []}\n",
    "\n",
    "    # prepare model for training\n",
    "    model.train()\n",
    "\n",
    "    #######################\n",
    "    # train the model #\n",
    "    #######################\n",
    "    for batch in tqdm(train_dl):\n",
    "        for i in range(len(batch['image'])):\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(batch['image'][i].to(config['device']))\n",
    "            # calculate the loss\n",
    "            # loss = criterion(output, batch[''][i].to(config['device']))\n",
    "            loss = F.cross_entropy(output, batch['image'][i].to(config['device'])).item()\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # record learning rate\n",
    "            lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            # update running training loss\n",
    "            train_loss += loss.item()*batch['image'][i].size(0)\n",
    "\n",
    "    ######################\n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for batch in val_dl:\n",
    "        # compute predicted outputs by passing inputs to the model\n",
    "        output = model(batch['image'][0].to(config['device']))\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, batch[''][0].to(config['device']))\n",
    "        # update running validation loss\n",
    "        valid_loss += loss.item()*batch['image'][0].size(0)\n",
    "\n",
    "    # print training/validation statistics\n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/(len(train_dl.dataset)*config[\"rot_num\"])\n",
    "    result['train_loss'] = train_loss\n",
    "    valid_loss = valid_loss/len(val_dl.dataset)\n",
    "    result['val_loss'] = valid_loss\n",
    "    leaning_rate = lrs\n",
    "    result['lrs'] = leaning_rate\n",
    "    history.append(result)\n",
    "\n",
    "    print('Epoch {:2d}: Learning Rate: {:.6f} Training Loss: {:.6f} Validation Loss:{:.6f}'.format(\n",
    "        epoch+1,\n",
    "        leaning_rate[-1],\n",
    "        train_loss,\n",
    "        valid_loss\n",
    "        ))\n",
    "\n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print(\"Validation loss decreased({:.6f}-->{:.6f}). Saving model ..\".format(\n",
    "        valid_loss_min,\n",
    "        valid_loss\n",
    "        ))\n",
    "        torch.save(model.state_dict(),\"model.pt\")\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "em-PUm2HXEKg"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# torch.save(model.state_dict(), '/home/va8800/ken_ai/v3_2/model_weights.pth')\n",
    "torch.save(model.state_dict(), '/home/STuser19/MID/v3/model_weights.pth')\n",
    "print(\"Save model weight successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rVmvw-feZRM"
   },
   "outputs": [],
   "source": [
    "def plot_losses(history):\n",
    "  train_losses = [x.get('train_loss') for x in history]\n",
    "  val_losses = [x['val_loss'] for x in history]\n",
    "  plt.plot(train_losses, '-bx')\n",
    "  plt.plot(val_losses, '-rx')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.ylabel('loss')\n",
    "  plt.legend(['Training', 'Validation'])\n",
    "  plt.title('Loss vs. No. of epochs');\n",
    "\n",
    "plot_losses(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VXM16izaeZW5"
   },
   "outputs": [],
   "source": [
    "def plot_lrs(history):\n",
    "  lrs = np.concatenate([x.get('lrs', []) for x in history])\n",
    "  plt.plot(lrs)\n",
    "  plt.xlabel('Batch no.')\n",
    "  plt.ylabel('Learning rate')\n",
    "  plt.title('Learning Rate vs. Batch no.');\n",
    "\n",
    "plot_lrs(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5L12W9QQ-y1"
   },
   "source": [
    "# Load your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rQRCaHFQ_Bd"
   },
   "outputs": [],
   "source": [
    "# weight_path = '/home/va8800/ken_ai/v3_2/model_weights.pth'\n",
    "weight_path = '/home/STuser19/MID/v3/model_weights.pth'\n",
    "\n",
    "checkpoint = torch.load(weight_path)\n",
    "model.eval()\n",
    "model.load_state_dict(checkpoint, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LhRSBI9rsQzD"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5_9PMTshxgh"
   },
   "source": [
    "# Testing part (**do not modify!!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wyulctAF6IAT"
   },
   "outputs": [],
   "source": [
    "class Testing_dataset(Dataset):\n",
    "  def __init__(self, config):\n",
    "    self.image_names = natsorted(glob(os.path.join(config['data_dir'], 'testing/rgb', '*.jpg')))\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.image_names)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # input\n",
    "    image_name = self.image_names[idx]\n",
    "    image = cv2.imread(image_name)\n",
    "    image = torch.from_numpy(image.transpose(2, 0 ,1))\n",
    "    image = image / 255\n",
    "\n",
    "    return {\n",
    "      'image_name': image_name,\n",
    "      'image': image,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_sBtvANBoi5"
   },
   "outputs": [],
   "source": [
    "test_ds = Testing_dataset(config)\n",
    "test_dl = DataLoader(test_ds, batch_size=1, shuffle=False, drop_last=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iv8KgYCVJWc7"
   },
   "outputs": [],
   "source": [
    "with open('./FreiHAND_pub_v2/golden_out.json', 'r') as f:\n",
    "  golden_out = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pCGAOevPCQvF"
   },
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model, golden_out):\n",
    "  print(f'Total {len(dataloader)} iterations to be tested.')\n",
    "\n",
    "  with torch.no_grad():\n",
    "    result = {}\n",
    "    loss = []\n",
    "    pixeldiff = []\n",
    "    for i, data in tqdm(enumerate(dataloader)):\n",
    "      image_name = data['image_name'][0][-22:]\n",
    "      # get golden ans and inputs\n",
    "      golden_ans = torch.from_numpy(np.array(golden_out[image_name])).float().cuda()\n",
    "      inputs = data['image'].cuda()\n",
    "\n",
    "      # get predicted output\n",
    "      outputs = model(inputs)\n",
    "\n",
    "      # get loss\n",
    "      l = criterion(outputs, golden_ans).cpu().numpy()\n",
    "      loss.append(l)\n",
    "      pixeldiff.append(np.sqrt(l) * 223)\n",
    "\n",
    "      pred = outputs.cpu().detach().numpy().tolist()\n",
    "      result.update({image_name: pred})\n",
    "\n",
    "  print('\\nTesting successfully!')\n",
    "  return result, loss, pixeldiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wzOAAKK4fxDw"
   },
   "outputs": [],
   "source": [
    "result, loss, pixeldiff = evaluate(test_dl, model, golden_out)\n",
    "\n",
    "with open(\"result.json\", \"w\") as outfile:\n",
    "  json.dump(result, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "okgGYyGcPZ5a"
   },
   "outputs": [],
   "source": [
    "with open('ID_result.csv', 'w') as f:\n",
    "  f.write('Id,Loss\\n')\n",
    "  for i, data in enumerate(pixeldiff):\n",
    "    f.write(str(i)+','+str(data)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qi_TysEclX69"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.move('ID_result.csv', '/home/STuser19/MID/v3/ID_result.csv')\n",
    "# shutil.move('ID_result.csv', '/home/va8800/ken_ai/v3_2/ID_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'FLOPS: {macs * 2 / 1e9} G, Params: {parm / 1e6} M., avg pixeldiff: {np.mean(pixeldiff)}, avg loss: {np.mean(loss)}')\n",
    "score = (macs * 2 / 1e9) * (parm / 1e6) * math.exp(np.mean(pixeldiff))\n",
    "print(\"ranking score: \", \"{:,}\".format(score))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "nKHgOCW7hAYS"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
