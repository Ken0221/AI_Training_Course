{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J93a2JElfqlj",
    "outputId": "73e52d48-6a39-436e-a3f7-b5d108b12027"
   },
   "outputs": [],
   "source": [
    "# !pip install thop\n",
    "# !pip install torchsummary\n",
    "# !pip install einops\n",
    "# !pip install -q kaggle\n",
    "# !pip install torch\n",
    "# !pip install numpy\n",
    "# !pip install opencv-python\n",
    "# !pip install matplotlib\n",
    "# !pip install natsort\n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "uJUnHoY3ag_1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from glob import glob\n",
    "from natsort import natsorted\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from thop import profile\n",
    "from thop import clever_format\n",
    "from torchsummary import summary\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.facecolor'] = '#ffffff'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: NVIDIA RTX A4000\n"
     ]
    }
   ],
   "source": [
    "result = torch.cuda.get_device_name(torch.cuda.current_device()) if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Current device: \" + result)\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDCdGBRHhNwR"
   },
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x4sCPVAkZjYp",
    "outputId": "62a1a022-ed50-4212-a3ae-cf4e9e8e1a77"
   },
   "outputs": [],
   "source": [
    "# !unzip DIV2K_train_HR.zip\n",
    "# !unzip DIV2K_valid_HR.zip\n",
    "# !unzip DIV2K_train_LR_bicubic_X3.zip\n",
    "# !unzip DIV2K_valid_LR_bicubic_X3.zip\n",
    "# !unzip DIV2k_train_LR_unknown_X3.zip\n",
    "# !unzip DIV2k_valid_LR_unknown_X3.zip\n",
    "\n",
    "# !unzip Set5.zip\n",
    "# !rm ./adarlab-ai-training.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gBEhrxWhRSR"
   },
   "source": [
    "# Checking Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "YRR5rwFsfwl4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/va8800/ken_ai/2024-ai-training-fianl-project\n",
      "3300\n",
      "250\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "\n",
    "img_train_data_dir_1 = './DIV2K_train_LR_bicubic/X3'\n",
    "img_train_data_dir_2 = './Flickr2K_LR_bicubic/train_X3'\n",
    "train_images = natsorted(glob(os.path.join(img_train_data_dir_1, '*.png'))) + natsorted(glob(os.path.join(img_train_data_dir_2, '*.png')))\n",
    "print(len(train_images))\n",
    "\n",
    "img_valid_data_dir_1 = './DIV2K_valid_LR_bicubic/X3'\n",
    "img_valid_data_dir_2 = './Flickr2K_LR_bicubic/val_X3'\n",
    "valid_images = natsorted(glob(os.path.join(img_valid_data_dir_1, '*.png'))) + natsorted(glob(os.path.join(img_valid_data_dir_2, '*.png')))\n",
    "print(len(valid_images))\n",
    "\n",
    "ans_train_data_dir_1 = './DIV2K_train_HR'\n",
    "ans_train_data_dir_2 = './Flickr2K_train_HR'\n",
    "train_ans = natsorted(glob(os.path.join(ans_train_data_dir_1, '*.png'))) + natsorted(glob(os.path.join(ans_train_data_dir_2, '*.png')))\n",
    "\n",
    "ans_valid_data_dir_1 = './DIV2K_valid_HR'\n",
    "ans_valid_data_dir_2 = './Flickr2K_val_HR'\n",
    "valid_ans = natsorted(glob(os.path.join(ans_valid_data_dir_1, '*.png'))) + natsorted(glob(os.path.join(ans_valid_data_dir_2, '*.png')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3462RLc7cxvd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9P5TGP47hDPR"
   },
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "9BDTfi-5fp5q"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"data_dir\": '/home/va8800/ken_ai/2024-ai-training-fianl-project',\n",
    "  \"data_num\": 900,\n",
    "  \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "  # you can set your own training configurations\n",
    "  \"batch_size\": 1,\n",
    "  \"learning_rate\": 0.0005,\n",
    "  \"n_epochs\": 100,\n",
    "  \"pic_num\": 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = None\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.RandomCrop(224),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "6RRZpsGTfejS"
   },
   "outputs": [],
   "source": [
    "class TrainImg(Dataset):\n",
    "  def __init__(self, config, set_type=\"train\", transform=None):\n",
    "    self.device = config[\"device\"]\n",
    "    self.transform = transform\n",
    "    self.set_type = set_type\n",
    "    ###########################your code here#############################\n",
    "    # get the image path list -> self.image_names\n",
    "    self.train_image_names = train_images\n",
    "    self.train_ans_names = train_ans\n",
    "    self.valid_image_names = valid_images\n",
    "    self.valid_ans_names = valid_ans\n",
    "\n",
    "    ########################################################################\n",
    "    if set_type == \"train\":\n",
    "        # n_start = 0\n",
    "        # n_end = 4\n",
    "        self.image_names = self.train_image_names\n",
    "        self.ans_names = self.train_ans_names\n",
    "  \n",
    "    elif set_type == \"val\":\n",
    "        # n_start = 4\n",
    "        # n_end = config['data_num']\n",
    "        self.image_names = self.valid_image_names\n",
    "        self.ans_names = self.valid_ans_names\n",
    "\n",
    "    # self.image_names = self.image_names[n_start:n_end]\n",
    "    # self.ans_names = self.ans_names[n_start:n_end]\n",
    "  \n",
    "    ########################################################################\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.image_names)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    ########################################################################\n",
    "    # get the input image step by step\n",
    "    # 1. read the image using cv2\n",
    "    # 2. transpose the dimension from [h, w, 3] to [3, h, w]\n",
    "    # 3. from numpy array to tensor\n",
    "    # 4. normalize the value from [0, 255] to [0, 1]\n",
    "    image_name = self.image_names[idx]\n",
    "    image = cv2.imread(image_name, cv2.IMREAD_COLOR)\n",
    "    image = np.transpose(image, (2, 0, 1))  # transpose the dimension from [h, w, 3] to [3, h, w]\n",
    "    image = torch.from_numpy(image).float() # from numpy array to tensor\n",
    "    # image = image / 255 # normalize the value from [0, 255] to [0, 1]\n",
    "    \n",
    "    ans_name = self.ans_names[idx]\n",
    "    ans = cv2.imread(ans_name, cv2.IMREAD_COLOR)\n",
    "    ans = np.transpose(ans, (2, 0, 1))  # transpose the dimension from [h, w, 3] to [3, h, w]\n",
    "    ans = torch.from_numpy(ans).float() # from numpy array to tensor\n",
    "    # image = image / 255 # normalize the value from [0, 255] to [0, 1]\n",
    "    \n",
    "    image_pic_arr = []\n",
    "    image_pic_arr.append(image)\n",
    "    \n",
    "    ans_pic_arr = []\n",
    "    ans_pic_arr.append(ans)\n",
    "    \n",
    "    length = config[\"pic_num\"] - 1\n",
    "    # print('Length:', length)\n",
    "    # if self.transform:\n",
    "    #     for i in range(length):\n",
    "    #         image_pic = self.transform((image))\n",
    "    #         image_pic_arr.append(image_pic)\n",
    "            \n",
    "    #         ans_pic = self.transform((ans))\n",
    "    #         ans_pic_arr.append(ans_pic)\n",
    "    # else:\n",
    "    #     for i in range(length):\n",
    "    #         image_pic_arr.append(image)\n",
    "    #         ans_pic_arr.append(ans)\n",
    "    \n",
    "    _, i_h, i_w = image.shape\n",
    "    _, a_h, a_w = ans.shape\n",
    "    if(self.set_type == \"train\" and i_h > 81 and i_w > 81):\n",
    "        for i in range(length):\n",
    "            # 定義隨機裁切區域，並保證裁切區域對應\n",
    "            top_img = random.randint(0, i_h - 81)\n",
    "            left_img = random.randint(0, i_w - 81)\n",
    "            image_crop = image[:, top_img:top_img + 81, left_img:left_img + 81]\n",
    "            \n",
    "            # 按比例裁切 ans 對應區域\n",
    "            top_ans = int(top_img * (a_h / i_h))\n",
    "            left_ans = int(left_img * (a_w / i_w))\n",
    "            ans_crop = ans[:, top_ans:top_ans + 243, left_ans:left_ans + 243]\n",
    "            \n",
    "            image_pic_arr.append(image_crop)\n",
    "            ans_pic_arr.append(ans_crop)\n",
    "    elif(i_h < 81 and i_w < 81):\n",
    "        print('image size is too small, image size:', i_h, i_w)\n",
    "    \n",
    "    \n",
    "    image = image / 255 # normalize the value from [0, 255] to [0, 1]\n",
    "    ans = ans / 255 # normalize the value from [0, 255] to [0, 1]\n",
    "\n",
    "    for i in range(length + 1):\n",
    "        image_pic_arr[i] = image_pic_arr[i] / 255\n",
    "        ans_pic_arr[i] = ans_pic_arr[i] / 255        \n",
    "    \n",
    "    return {\n",
    "        'image_name': image_name,\n",
    "        'image': image_pic_arr,\n",
    "        'ans': ans_pic_arr,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length:  3300\n",
      "Validation dataset length:  250\n"
     ]
    }
   ],
   "source": [
    "train_ds = TrainImg(config, set_type='train', transform=transform)\n",
    "val_ds = TrainImg(config, set_type='val')\n",
    "\n",
    "train_dl = DataLoader(train_ds, config[\"batch_size\"], shuffle=True, drop_last=True, num_workers=1)\n",
    "val_dl = DataLoader(val_ds, config[\"batch_size\"], shuffle=True, drop_last=True, num_workers=1)\n",
    "\n",
    "# print(\"Total dataset length: \", config['data_num'])\n",
    "print(\"Train dataset length: \", len(train_ds))\n",
    "print(\"Validation dataset length: \", len(val_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKHgOCW7hAYS"
   },
   "source": [
    "# Show images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_img(image, vis=None, color_fixed=None, linewidth=1, img_order='rgb', draw_kp=True, kp_style=None):\n",
    "  \"\"\" Inpaints a hand stick figure into a matplotlib figure.\n",
    "  image:    original image input\n",
    "  coords_hw:  predicted keypoint (non normalized) -> [0, 224)\n",
    "  \"\"\"\n",
    "  if kp_style is None:\n",
    "    # kp_style[0] for circle radius, kp_style[1] for circle point thickness\n",
    "    kp_style = (1, 2)\n",
    "\n",
    "  # if image have four dimension like [1. 224. 224. 3] then squeeze to [3. 224. 3]\n",
    "  image = np.squeeze(image)\n",
    "\n",
    "  if len(image.shape) == 2:\n",
    "    image = np.expand_dims(image, 2)\n",
    "  s = image.shape\n",
    "  assert len(s) == 3, \"This only works for single images.\"\n",
    "\n",
    "  convert_to_uint8 = False\n",
    "\n",
    "  if s[2] == 1:\n",
    "    # grayscale case\n",
    "    image = (image - np.min(image)) / (np.max(image) - np.min(image) + 1e-4)\n",
    "    image = np.tile(image, [1, 1, 3])\n",
    "    pass\n",
    "\n",
    "  elif s[2] == 3:\n",
    "    # RGB case\n",
    "    if image.dtype == np.uint8:\n",
    "        convert_to_uint8 = True\n",
    "        image = image.astype('float64') / 255.0\n",
    "    elif image.dtype == np.float32:\n",
    "        # convert to gray image\n",
    "        image = np.mean(image, axis=2)\n",
    "        image = (image - np.min(image)) / (np.max(image) - np.min(image) + 1e-4)\n",
    "        image = np.expand_dims(image, 2)\n",
    "        image = np.tile(image, [1, 1, 3])\n",
    "  else:\n",
    "    assert 0, \"Unknown image dimensions.\"\n",
    "\n",
    "  colors = np.array(\n",
    "    [[0.4, 0.4, 0.4],\n",
    "    [0.4, 0.0, 0.0],\n",
    "    [0.6, 0.0, 0.0],\n",
    "    [0.8, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [0.4, 0.4, 0.0],\n",
    "    [0.6, 0.6, 0.0],\n",
    "    [0.8, 0.8, 0.0],\n",
    "    [1.0, 1.0, 0.0],\n",
    "    [0.0, 0.4, 0.2],\n",
    "    [0.0, 0.6, 0.3],\n",
    "    [0.0, 0.8, 0.4],\n",
    "    [0.0, 1.0, 0.5],\n",
    "    [0.0, 0.2, 0.4],\n",
    "    [0.0, 0.3, 0.6],\n",
    "    [0.0, 0.4, 0.8],\n",
    "    [0.0, 0.5, 1.0],\n",
    "    [0.4, 0.0, 0.4],\n",
    "    [0.6, 0.0, 0.6],\n",
    "    [0.7, 0.0, 0.8],\n",
    "    [1.0, 0.0, 1.0]]\n",
    "  )\n",
    "\n",
    "  if img_order == 'rgb':\n",
    "    # cv2 operation under BGR\n",
    "    colors = colors[:, ::-1]\n",
    "\n",
    "  color_map = {\n",
    "    'k': np.array([0.0, 0.0, 0.0]),\n",
    "    'w': np.array([1.0, 1.0, 1.0]),\n",
    "    'b': np.array([0.0, 0.0, 1.0]),\n",
    "    'g': np.array([0.0, 1.0, 0.0]),\n",
    "    'r': np.array([1.0, 0.0, 0.0]),\n",
    "    'm': np.array([1.0, 1.0, 0.0]),\n",
    "    'c': np.array([0.0, 1.0, 1.0])\n",
    "  }\n",
    "\n",
    "  if convert_to_uint8:\n",
    "    image = (image * 255).astype('uint8')\n",
    "\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_iter = iter(train_dl)\n",
    "# batch = next(batch_iter)\n",
    "\n",
    "# imgs = batch['image'] #return[img1, img2, img3]\n",
    "# print(batch['image'])\n",
    "# pic_num = 4\n",
    "# fig, axes = plt.subplots(len(imgs), pic_num, figsize=(15, 10))\n",
    "# for i in range(len(imgs)):\n",
    "#     for j in range(pic_num):\n",
    "#         # 將圖像和關鍵點轉換為 NumPy 格式\n",
    "#         img_np = imgs[i][j].permute(1, 2, 0).numpy()\n",
    "\n",
    "#         # 使用 draw_hand 函數繪製關鍵點\n",
    "#         trainimg = draw_img\n",
    "        \n",
    "#         # 顯示結果\n",
    "#         axes[i, j].imshow(trainimg)\n",
    "#         # axes[i, j].axis('off')\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eb_pWxc2xmOM"
   },
   "source": [
    "# Model\n",
    "\n",
    "+ Model Specifications:\n",
    "  + Input: **`[B, 3, 224, 224]`**\n",
    "  + Output: **`[B, 21, 2]`** --> 21 for the num of the landmarks, 2 for the coordinates in (x, y) format\n",
    "  + Layer: You can build up your own model architecture with no limitations.\n",
    "  + Cost: The computational cost (FLOPs) may not over **`20 GFLOPs`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class RCAB(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(RCAB, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channel, channel, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(channel, channel, kernel_size=3, padding=1)\n",
    "        self.ca = ChannelAttention(channel, reduction)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.conv1(x)\n",
    "        res = self.relu(res)\n",
    "        res = self.conv2(res)\n",
    "        res = self.ca(res)\n",
    "        return x + res\n",
    "\n",
    "class RIR(nn.Module):\n",
    "    def __init__(self, channel, num_rcab=6):\n",
    "        super(RIR, self).__init__()\n",
    "        self.rcabs = nn.Sequential(*[RCAB(channel) for _ in range(num_rcab)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.rcabs(x)\n",
    "\n",
    "class SuperResolution(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SuperResolution, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 18, kernel_size=5, stride=1, padding=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(18, 27, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "        self.rir = RIR(27)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(27, 18, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "        self.PS = nn.PixelShuffle(3)\n",
    "        self.prelu = nn.PReLU()\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(2, 3, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial feature extraction\n",
    "        x1 = self.prelu(self.conv1(x))\n",
    "        x = self.prelu(self.conv2(x1))\n",
    "        # First RIR block\n",
    "        x = self.rir(x)\n",
    "        x = self.prelu(self.conv3(x))\n",
    "\n",
    "        x = x + x1\n",
    "\n",
    "        x = self.PS(x)\n",
    "        \n",
    "        # Final convolution for channel adjustment\n",
    "        x = self.final_conv(x)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "w9mqC8nZW7Tg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SuperResolution(\n",
      "  (conv1): Conv2d(3, 18, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(18, 27, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (rir): RIR(\n",
      "    (rcabs): Sequential(\n",
      "      (0): RCAB(\n",
      "        (conv1): Conv2d(27, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(27, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (ca): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=27, out_features=1, bias=False)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=1, out_features=27, bias=False)\n",
      "            (3): Sigmoid()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): RCAB(\n",
      "        (conv1): Conv2d(27, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(27, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (ca): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=27, out_features=1, bias=False)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=1, out_features=27, bias=False)\n",
      "            (3): Sigmoid()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): RCAB(\n",
      "        (conv1): Conv2d(27, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(27, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (ca): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=27, out_features=1, bias=False)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=1, out_features=27, bias=False)\n",
      "            (3): Sigmoid()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): RCAB(\n",
      "        (conv1): Conv2d(27, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(27, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (ca): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=27, out_features=1, bias=False)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=1, out_features=27, bias=False)\n",
      "            (3): Sigmoid()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): RCAB(\n",
      "        (conv1): Conv2d(27, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(27, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (ca): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=27, out_features=1, bias=False)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=1, out_features=27, bias=False)\n",
      "            (3): Sigmoid()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): RCAB(\n",
      "        (conv1): Conv2d(27, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(27, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (ca): ChannelAttention(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=27, out_features=1, bias=False)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=1, out_features=27, bias=False)\n",
      "            (3): Sigmoid()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(27, 18, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (PS): PixelShuffle(upscale_factor=3)\n",
      "  (prelu): PReLU(num_parameters=1)\n",
      "  (final_conv): Conv2d(2, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# from model import SuperResolution\n",
    "\n",
    "Net = SuperResolution()\n",
    "print(Net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5cExNrO9Ad6"
   },
   "source": [
    "# Testing Model Computational Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "yTf0tYV2eFSh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================\n",
      "FLOPs : 10.717103052 G\n",
      "PARAMS : 0.081823 M \n",
      "============================\n",
      "Your FLOPs is smaller than 11.1 G.\n",
      "You will get this 15 points.\n",
      "Congratulations !!!\n"
     ]
    }
   ],
   "source": [
    "# # pseudo image\n",
    "# image = torch.rand(1, 3, 224, 224).cuda()\n",
    "\n",
    "# # define your model\n",
    "model = Net.to(config[\"device\"])\n",
    "\n",
    "# out = model(image)\n",
    "\n",
    "# # torchsummary report\n",
    "# summary(model, input_size=(3, 224, 224))\n",
    "# print(f'From input shape: {image.shape} to output shape: {out.shape}')\n",
    "\n",
    "# # thop report\n",
    "# macs, parm = profile(model, inputs=(image, ))\n",
    "# print(f'FLOPS: {macs * 2 / 1e9} G, Params: {parm / 1e6} M.')\n",
    "img = torch.randn(1, 3, 256, 256).to(config['device'])\n",
    "macs, params = profile(model, inputs=(img, ), verbose=False)\n",
    "flops = macs * 2 / 1e9  # G\n",
    "params = params / 1e6   # M\n",
    "print('============================')\n",
    "print(f'FLOPs : { flops } G')\n",
    "print(f'PARAMS : { params } M ')\n",
    "print('============================')\n",
    "if flops < 11.1:\n",
    "    print('Your FLOPs is smaller than 11.1 G.')\n",
    "    print('You will get this 15 points.')\n",
    "    print('Congratulations !!!')\n",
    "else:\n",
    "    print('Your FLOPs is larger than 11.1 G.')\n",
    "    print('You will not get this 10 points.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BB7B2iQI8IQb"
   },
   "source": [
    "# Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "m0KvUVwb8NjQ"
   },
   "outputs": [],
   "source": [
    "# class CMSELoss(nn.Module):\n",
    "#   \"\"\"\n",
    "#   Coordinate MSE loss\n",
    "#   input :\n",
    "#     y_pred = b, 21, 2   (coordinate of 21 keypoints)\n",
    "#     y_true = b, 21, 2   (keypoints, (y, x))\n",
    "#   \"\"\"\n",
    "#   def __init__(self):\n",
    "#     super().__init__()\n",
    "#     self.loss = nn.MSELoss()\n",
    "\n",
    "#   def forward(self, y_pred, y_true):\n",
    "#     y_true = torch.flip(y_true, [2]) # flip (y, x) to (x, y)\n",
    "#     return self.loss(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "AmZhlbbPhKJU"
   },
   "outputs": [],
   "source": [
    "# criterion = CMSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xGp8--mK9o8U"
   },
   "source": [
    "# Optimizer and Scheduler (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "rHIG6WpB9pyL"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "# optimizer = torch.optim.Adadelta(model.parameters(), lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "0HGT7xWcO59p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330000.0\n"
     ]
    }
   ],
   "source": [
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.002, steps_per_epoch=len(train_dl), epochs=config[\"n_epochs\"])\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, steps_per_epoch=len(train_dl), epochs=config[\"n_epochs\"], div_factor=2, final_div_factor=5, pct_start=0.09)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = config[\"pic_num\"] * (len(train_dl.dataset) / config[\"batch_size\"]) * config[\"n_epochs\"], eta_min = 0.00005)\n",
    "print((len(train_dl.dataset) / config[\"batch_size\"]) * config[\"n_epochs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvispd49hh0p"
   },
   "source": [
    "# Training\n",
    "  + Record the **`loss / epoch`** learning curve\n",
    "  + If using learning rate scheduler, record the **`lr / epoch`** curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_path = '/home/va8800/ken_ai/model_weights.pth'\n",
    "# weight_path = '/home/STuser19/MID/model_weights.pth'\n",
    "\n",
    "# checkpoint = torch.load(weight_path)\n",
    "# model.load_state_dict(checkpoint, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CxgfsfnDeUft"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|███████████████████████████████████████████▍  | 3115/3300 [11:33<00:44,  4.17it/s]"
     ]
    }
   ],
   "source": [
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            # initialize tracker for minimum validation loss\n",
    "# valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "valid_loss_min = np.inf # set initial \"min\" to infinity\n",
    "\n",
    "# initialize history for recording what we want to know\n",
    "history = []\n",
    "\n",
    "for epoch in range(config[\"n_epochs\"]):\n",
    "    # monitor training loss, validation loss and learning rate\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    lrs    = []\n",
    "    result = {'train_loss': [], 'val_loss': [], 'lrs': []}\n",
    "\n",
    "    # prepare model for training\n",
    "    model.train()\n",
    "\n",
    "    #######################\n",
    "    # train the model #\n",
    "    #######################\n",
    "    for batch in tqdm(train_dl):\n",
    "        for i in range(len(batch['image'])):\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            # print(batch['image'][i].shape)\n",
    "            output = model(batch['image'][i].to(config['device']))\n",
    "            # print(output.shape)\n",
    "            # print(batch['ans'][i].shape)\n",
    "            if(output.shape != batch['ans'][i].shape):\n",
    "                output = torch.nn.functional.interpolate(output, size=batch['ans'][i].shape[-2:], mode='bilinear', align_corners=False)\n",
    "            # calculate the loss\n",
    "            loss = ((output - batch['ans'][i].to(config['device'])) ** 2).mean()\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # record learning rate\n",
    "            lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            # update running training loss\n",
    "            train_loss += loss.item()*batch['image'][i].size(0)\n",
    "\n",
    "    ######################\n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for batch in val_dl:\n",
    "        # compute predicted outputs by passing inputs to the model\n",
    "        output = model(batch['image'][0].to(config['device']))\n",
    "        if(output.shape != batch['ans'][i].shape):\n",
    "                output = torch.nn.functional.interpolate(output, size=batch['ans'][i].shape[-2:], mode='bilinear', align_corners=False)\n",
    "        # calculate the loss\n",
    "        # loss = criterion(output, batch[''][0].to(config['device']))\n",
    "        loss = ((output - batch['ans'][i].to(config['device'])) ** 2).mean()\n",
    "\n",
    "        # update running validation loss\n",
    "        valid_loss += loss.item()*batch['image'][0].size(0)\n",
    "\n",
    "    # print training/validation statistics\n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/(len(train_dl.dataset)*config[\"pic_num\"])\n",
    "    result['train_loss'] = train_loss\n",
    "    valid_loss = valid_loss/len(val_dl.dataset)\n",
    "    result['val_loss'] = valid_loss\n",
    "    leaning_rate = lrs\n",
    "    result['lrs'] = leaning_rate\n",
    "    history.append(result)\n",
    "\n",
    "    print('Epoch {:2d}: Learning Rate: {:.6f} Training Loss: {:.6f} Validation Loss:{:.6f}'.format(\n",
    "        epoch+1,\n",
    "        leaning_rate[-1],\n",
    "        train_loss,\n",
    "        valid_loss\n",
    "        ))\n",
    "\n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print(\"Validation loss decreased({:.6f}-->{:.6f}). Saving model ..\".format(\n",
    "        valid_loss_min,\n",
    "        valid_loss\n",
    "        ))\n",
    "        torch.save(model.state_dict(),\"model.pt\")\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "em-PUm2HXEKg"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "torch.save(model.state_dict(), '/home/va8800/ken_ai/2024-ai-training-fianl-project/model.pth')\n",
    "# torch.save(model.state_dict(), '/home/STuser19/MID/v3/model.pth')\n",
    "print(\"Save model weight successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rVmvw-feZRM"
   },
   "outputs": [],
   "source": [
    "def plot_losses(history):\n",
    "  train_losses = [x.get('train_loss') for x in history]\n",
    "  val_losses = [x['val_loss'] for x in history]\n",
    "  plt.plot(train_losses, '-bx')\n",
    "  plt.plot(val_losses, '-rx')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.ylabel('loss')\n",
    "  plt.legend(['Training', 'Validation'])\n",
    "  plt.title('Loss vs. No. of epochs');\n",
    "\n",
    "plot_losses(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VXM16izaeZW5"
   },
   "outputs": [],
   "source": [
    "def plot_lrs(history):\n",
    "  lrs = np.concatenate([x.get('lrs', []) for x in history])\n",
    "  plt.plot(lrs)\n",
    "  plt.xlabel('Batch no.')\n",
    "  plt.ylabel('Learning rate')\n",
    "  plt.title('Learning Rate vs. Batch no.');\n",
    "\n",
    "plot_lrs(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5L12W9QQ-y1"
   },
   "source": [
    "# Load your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rQRCaHFQ_Bd"
   },
   "outputs": [],
   "source": [
    "weight_path = '/home/va8800/ken_ai/2024-ai-training-fianl-project/model.pth'\n",
    "# weight_path = '/home/STuser19/MID/v3/model_weights.pth'\n",
    "\n",
    "checkpoint = torch.load(weight_path)\n",
    "model.eval()\n",
    "model.load_state_dict(checkpoint, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LhRSBI9rsQzD"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "nKHgOCW7hAYS"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
