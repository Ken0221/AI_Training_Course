{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J93a2JElfqlj",
    "outputId": "73e52d48-6a39-436e-a3f7-b5d108b12027"
   },
   "outputs": [],
   "source": [
    "# !pip install thop\n",
    "# !pip install torchsummary\n",
    "# !pip install einops\n",
    "# !pip install -q kaggle\n",
    "# !pip install torch\n",
    "# !pip install numpy\n",
    "# !pip install opencv-python\n",
    "# !pip install matplotlib\n",
    "# !pip install natsort\n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "uJUnHoY3ag_1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from glob import glob\n",
    "from natsort import natsorted\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from thop import profile\n",
    "from thop import clever_format\n",
    "from torchsummary import summary\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.facecolor'] = '#ffffff'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: NVIDIA RTX A4000\n"
     ]
    }
   ],
   "source": [
    "result = torch.cuda.get_device_name(torch.cuda.current_device()) if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Current device: \" + result)\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDCdGBRHhNwR"
   },
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x4sCPVAkZjYp",
    "outputId": "62a1a022-ed50-4212-a3ae-cf4e9e8e1a77"
   },
   "outputs": [],
   "source": [
    "# !unzip DIV2K_train_HR.zip\n",
    "# !unzip DIV2K_valid_HR.zip\n",
    "# !unzip DIV2K_train_LR_bicubic_X3.zip\n",
    "# !unzip DIV2K_valid_LR_bicubic_X3.zip\n",
    "# !unzip DIV2k_train_LR_unknown_X3.zip\n",
    "# !unzip DIV2k_valid_LR_unknown_X3.zip\n",
    "\n",
    "# !unzip Set5.zip\n",
    "# !rm ./adarlab-ai-training.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gBEhrxWhRSR"
   },
   "source": [
    "# Checking Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "YRR5rwFsfwl4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/va8800/ken_ai/2024-ai-training-fianl-project\n",
      "3300\n",
      "250\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "\n",
    "img_train_data_dir_1 = './DIV2K_train_LR_bicubic/X3'\n",
    "img_train_data_dir_2 = './Flickr2K_LR_bicubic/train_X3'\n",
    "train_images = natsorted(glob(os.path.join(img_train_data_dir_1, '*.png'))) + natsorted(glob(os.path.join(img_train_data_dir_2, '*.png')))\n",
    "print(len(train_images))\n",
    "\n",
    "img_valid_data_dir_1 = './DIV2K_valid_LR_bicubic/X3'\n",
    "img_valid_data_dir_2 = './Flickr2K_LR_bicubic/val_X3'\n",
    "valid_images = natsorted(glob(os.path.join(img_valid_data_dir_1, '*.png'))) + natsorted(glob(os.path.join(img_valid_data_dir_2, '*.png')))\n",
    "print(len(valid_images))\n",
    "\n",
    "ans_train_data_dir_1 = './DIV2K_train_HR'\n",
    "ans_train_data_dir_2 = './Flickr2K_train_HR'\n",
    "train_ans = natsorted(glob(os.path.join(ans_train_data_dir_1, '*.png'))) + natsorted(glob(os.path.join(ans_train_data_dir_2, '*.png')))\n",
    "\n",
    "ans_valid_data_dir_1 = './DIV2K_valid_HR'\n",
    "ans_valid_data_dir_2 = './Flickr2K_val_HR'\n",
    "valid_ans = natsorted(glob(os.path.join(ans_valid_data_dir_1, '*.png'))) + natsorted(glob(os.path.join(ans_valid_data_dir_2, '*.png')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3462RLc7cxvd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9P5TGP47hDPR"
   },
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "9BDTfi-5fp5q"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"data_dir\": '/home/va8800/ken_ai/2024-ai-training-fianl-project',\n",
    "  \"data_num\": 900,\n",
    "  \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "  # you can set your own training configurations\n",
    "  \"batch_size\": 33,\n",
    "  \"learning_rate\": 0.0005,\n",
    "  \"n_epochs\": 50,\n",
    "  \"pic_num\": 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = None\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.RandomCrop(224),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "6RRZpsGTfejS"
   },
   "outputs": [],
   "source": [
    "class TrainImg(Dataset):\n",
    "  def __init__(self, config, set_type=\"train\", transform=None):\n",
    "    self.device = config[\"device\"]\n",
    "    self.transform = transform\n",
    "    self.set_type = set_type\n",
    "    ###########################your code here#############################\n",
    "    # get the image path list -> self.image_names\n",
    "    self.train_image_names = train_images\n",
    "    self.train_ans_names = train_ans\n",
    "    self.valid_image_names = valid_images\n",
    "    self.valid_ans_names = valid_ans\n",
    "\n",
    "    ########################################################################\n",
    "    if set_type == \"train\":\n",
    "        # n_start = 0\n",
    "        # n_end = 4\n",
    "        self.image_names = self.train_image_names\n",
    "        self.ans_names = self.train_ans_names\n",
    "  \n",
    "    elif set_type == \"val\":\n",
    "        # n_start = 4\n",
    "        # n_end = config['data_num']\n",
    "        self.image_names = self.valid_image_names\n",
    "        self.ans_names = self.valid_ans_names\n",
    "\n",
    "    # self.image_names = self.image_names[n_start:n_end]\n",
    "    # self.ans_names = self.ans_names[n_start:n_end]\n",
    "  \n",
    "    ########################################################################\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.image_names)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    ########################################################################\n",
    "    # get the input image step by step\n",
    "    # 1. read the image using cv2\n",
    "    # 2. transpose the dimension from [h, w, 3] to [3, h, w]\n",
    "    # 3. from numpy array to tensor\n",
    "    # 4. normalize the value from [0, 255] to [0, 1]\n",
    "    image_name = self.image_names[idx]\n",
    "    image = cv2.imread(image_name, cv2.IMREAD_COLOR)\n",
    "    image = np.transpose(image, (2, 0, 1))  # transpose the dimension from [h, w, 3] to [3, h, w]\n",
    "    image = torch.from_numpy(image).float() # from numpy array to tensor\n",
    "    # image = image / 255 # normalize the value from [0, 255] to [0, 1]\n",
    "    \n",
    "    ans_name = self.ans_names[idx]\n",
    "    ans = cv2.imread(ans_name, cv2.IMREAD_COLOR)\n",
    "    ans = np.transpose(ans, (2, 0, 1))  # transpose the dimension from [h, w, 3] to [3, h, w]\n",
    "    ans = torch.from_numpy(ans).float() # from numpy array to tensor\n",
    "    # image = image / 255 # normalize the value from [0, 255] to [0, 1]\n",
    "    \n",
    "    image_pic_arr = []\n",
    "    #image_pic_arr.append(image)\n",
    "    \n",
    "    ans_pic_arr = []\n",
    "    #ans_pic_arr.append(ans)\n",
    "    \n",
    "    length = config[\"pic_num\"]\n",
    "    # print('Length:', length)\n",
    "    # if self.transform:\n",
    "    #     for i in range(length):\n",
    "    #         image_pic = self.transform((image))\n",
    "    #         image_pic_arr.append(image_pic)\n",
    "            \n",
    "    #         ans_pic = self.transform((ans))\n",
    "    #         ans_pic_arr.append(ans_pic)\n",
    "    # else:\n",
    "    #     for i in range(length):\n",
    "    #         image_pic_arr.append(image)\n",
    "    #         ans_pic_arr.append(ans)\n",
    "    \n",
    "    _, i_h, i_w = image.shape\n",
    "    _, a_h, a_w = ans.shape\n",
    "    if(self.set_type == \"train\"):\n",
    "        if(i_h > 81 and i_w > 81):\n",
    "            for i in range(length):\n",
    "                # 定義隨機裁切區域，並保證裁切區域對應\n",
    "                top_img = random.randint(0, i_h - 81)\n",
    "                left_img = random.randint(0, i_w - 81)\n",
    "                image_crop = image[:, top_img:top_img + 81, left_img:left_img + 81]\n",
    "            \n",
    "                # 按比例裁切 ans 對應區域\n",
    "                top_ans = int(top_img * (a_h / i_h))\n",
    "                left_ans = int(left_img * (a_w / i_w))\n",
    "                ans_crop = ans[:, top_ans:top_ans + 243, left_ans:left_ans + 243]\n",
    "            \n",
    "                image_pic_arr.append(image_crop)\n",
    "                ans_pic_arr.append(ans_crop)\n",
    "        else:\n",
    "            print('image size is too small, image size:', i_h, i_w)\n",
    "    \n",
    "        image = image / 255 # normalize the value from [0, 255] to [0, 1]\n",
    "        ans = ans / 255 # normalize the value from [0, 255] to [0, 1]\n",
    "\n",
    "        for i in range(length):\n",
    "            image_pic_arr[i] = image_pic_arr[i] / 255\n",
    "            ans_pic_arr[i] = ans_pic_arr[i] / 255        \n",
    "    else:\n",
    "        image_pic_arr.append(image)\n",
    "        ans_pic_arr.append(ans)\n",
    "        image_pic_arr[0] = image_pic_arr[0] / 255\n",
    "        ans_pic_arr[0] = ans_pic_arr[0] / 255 \n",
    "        \n",
    "    return {\n",
    "        'image_name': image_name,\n",
    "        'image': image_pic_arr,\n",
    "        'ans': ans_pic_arr,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length:  3300\n",
      "Validation dataset length:  250\n"
     ]
    }
   ],
   "source": [
    "train_ds = TrainImg(config, set_type='train', transform=transform)\n",
    "val_ds = TrainImg(config, set_type='val')\n",
    "\n",
    "train_dl = DataLoader(train_ds, config[\"batch_size\"], shuffle=True, drop_last=True, num_workers=5)\n",
    "val_dl = DataLoader(val_ds, 1, shuffle=True, drop_last=True, num_workers=5)\n",
    "\n",
    "# print(\"Total dataset length: \", config['data_num'])\n",
    "print(\"Train dataset length: \", len(train_ds))\n",
    "print(\"Validation dataset length: \", len(val_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKHgOCW7hAYS"
   },
   "source": [
    "# Show images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_img(image, vis=None, color_fixed=None, linewidth=1, img_order='rgb', draw_kp=True, kp_style=None):\n",
    "  \"\"\" Inpaints a hand stick figure into a matplotlib figure.\n",
    "  image:    original image input\n",
    "  coords_hw:  predicted keypoint (non normalized) -> [0, 224)\n",
    "  \"\"\"\n",
    "  if kp_style is None:\n",
    "    # kp_style[0] for circle radius, kp_style[1] for circle point thickness\n",
    "    kp_style = (1, 2)\n",
    "\n",
    "  # if image have four dimension like [1. 224. 224. 3] then squeeze to [3. 224. 3]\n",
    "  image = np.squeeze(image)\n",
    "\n",
    "  if len(image.shape) == 2:\n",
    "    image = np.expand_dims(image, 2)\n",
    "  s = image.shape\n",
    "  assert len(s) == 3, \"This only works for single images.\"\n",
    "\n",
    "  convert_to_uint8 = False\n",
    "\n",
    "  if s[2] == 1:\n",
    "    # grayscale case\n",
    "    image = (image - np.min(image)) / (np.max(image) - np.min(image) + 1e-4)\n",
    "    image = np.tile(image, [1, 1, 3])\n",
    "    pass\n",
    "\n",
    "  elif s[2] == 3:\n",
    "    # RGB case\n",
    "    if image.dtype == np.uint8:\n",
    "        convert_to_uint8 = True\n",
    "        image = image.astype('float64') / 255.0\n",
    "    elif image.dtype == np.float32:\n",
    "        # convert to gray image\n",
    "        image = np.mean(image, axis=2)\n",
    "        image = (image - np.min(image)) / (np.max(image) - np.min(image) + 1e-4)\n",
    "        image = np.expand_dims(image, 2)\n",
    "        image = np.tile(image, [1, 1, 3])\n",
    "  else:\n",
    "    assert 0, \"Unknown image dimensions.\"\n",
    "\n",
    "  colors = np.array(\n",
    "    [[0.4, 0.4, 0.4],\n",
    "    [0.4, 0.0, 0.0],\n",
    "    [0.6, 0.0, 0.0],\n",
    "    [0.8, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [0.4, 0.4, 0.0],\n",
    "    [0.6, 0.6, 0.0],\n",
    "    [0.8, 0.8, 0.0],\n",
    "    [1.0, 1.0, 0.0],\n",
    "    [0.0, 0.4, 0.2],\n",
    "    [0.0, 0.6, 0.3],\n",
    "    [0.0, 0.8, 0.4],\n",
    "    [0.0, 1.0, 0.5],\n",
    "    [0.0, 0.2, 0.4],\n",
    "    [0.0, 0.3, 0.6],\n",
    "    [0.0, 0.4, 0.8],\n",
    "    [0.0, 0.5, 1.0],\n",
    "    [0.4, 0.0, 0.4],\n",
    "    [0.6, 0.0, 0.6],\n",
    "    [0.7, 0.0, 0.8],\n",
    "    [1.0, 0.0, 1.0]]\n",
    "  )\n",
    "\n",
    "  if img_order == 'rgb':\n",
    "    # cv2 operation under BGR\n",
    "    colors = colors[:, ::-1]\n",
    "\n",
    "  color_map = {\n",
    "    'k': np.array([0.0, 0.0, 0.0]),\n",
    "    'w': np.array([1.0, 1.0, 1.0]),\n",
    "    'b': np.array([0.0, 0.0, 1.0]),\n",
    "    'g': np.array([0.0, 1.0, 0.0]),\n",
    "    'r': np.array([1.0, 0.0, 0.0]),\n",
    "    'm': np.array([1.0, 1.0, 0.0]),\n",
    "    'c': np.array([0.0, 1.0, 1.0])\n",
    "  }\n",
    "\n",
    "  if convert_to_uint8:\n",
    "    image = (image * 255).astype('uint8')\n",
    "\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_iter = iter(train_dl)\n",
    "# batch = next(batch_iter)\n",
    "\n",
    "# imgs = batch['image'] #return[img1, img2, img3]\n",
    "# print(batch['image'])\n",
    "# pic_num = 4\n",
    "# fig, axes = plt.subplots(len(imgs), pic_num, figsize=(15, 10))\n",
    "# for i in range(len(imgs)):\n",
    "#     for j in range(pic_num):\n",
    "#         # 將圖像和關鍵點轉換為 NumPy 格式\n",
    "#         img_np = imgs[i][j].permute(1, 2, 0).numpy()\n",
    "\n",
    "#         # 使用 draw_hand 函數繪製關鍵點\n",
    "#         trainimg = draw_img\n",
    "        \n",
    "#         # 顯示結果\n",
    "#         axes[i, j].imshow(trainimg)\n",
    "#         # axes[i, j].axis('off')\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eb_pWxc2xmOM"
   },
   "source": [
    "# Model\n",
    "\n",
    "+ Model Specifications:\n",
    "  + Input: **`[B, 3, 224, 224]`**\n",
    "  + Output: **`[B, 21, 2]`** --> 21 for the num of the landmarks, 2 for the coordinates in (x, y) format\n",
    "  + Layer: You can build up your own model architecture with no limitations.\n",
    "  + Cost: The computational cost (FLOPs) may not over **`20 GFLOPs`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "w9mqC8nZW7Tg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SuperResolution(\n",
      "  (conv1): Conv2d(3, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv1_1): Conv2d(54, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(54, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv4): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv5): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv6): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv7): Conv2d(36, 36, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv8): Conv2d(36, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv9): Conv2d(54, 54, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (PS): PixelShuffle(upscale_factor=3)\n",
      "  (PRelu): PReLU(num_parameters=1)\n",
      "  (convf): Conv2d(6, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from model_new import SuperResolution\n",
    "\n",
    "Net = SuperResolution()\n",
    "print(Net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5cExNrO9Ad6"
   },
   "source": [
    "# Testing Model Computational Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "yTf0tYV2eFSh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================\n",
      "FLOPs : 9.689628672 G\n",
      "PARAMS : 0.072688 M \n",
      "============================\n",
      "Your FLOPs is smaller than 11.1 G.\n",
      "You will get this 15 points.\n",
      "Congratulations !!!\n"
     ]
    }
   ],
   "source": [
    "# # pseudo image\n",
    "# image = torch.rand(1, 3, 224, 224).cuda()\n",
    "\n",
    "# # define your model\n",
    "model = Net.to(config[\"device\"])\n",
    "\n",
    "# out = model(image)\n",
    "\n",
    "# # torchsummary report\n",
    "# summary(model, input_size=(3, 224, 224))\n",
    "# print(f'From input shape: {image.shape} to output shape: {out.shape}')\n",
    "\n",
    "# # thop report\n",
    "# macs, parm = profile(model, inputs=(image, ))\n",
    "# print(f'FLOPS: {macs * 2 / 1e9} G, Params: {parm / 1e6} M.')\n",
    "img = torch.randn(1, 3, 256, 256).to(config['device'])\n",
    "macs, params = profile(model, inputs=(img, ), verbose=False)\n",
    "flops = macs * 2 / 1e9  # G\n",
    "params = params / 1e6   # M\n",
    "print('============================')\n",
    "print(f'FLOPs : { flops } G')\n",
    "print(f'PARAMS : { params } M ')\n",
    "print('============================')\n",
    "if flops < 11.1:\n",
    "    print('Your FLOPs is smaller than 11.1 G.')\n",
    "    print('You will get this 15 points.')\n",
    "    print('Congratulations !!!')\n",
    "else:\n",
    "    print('Your FLOPs is larger than 11.1 G.')\n",
    "    print('You will not get this 10 points.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BB7B2iQI8IQb"
   },
   "source": [
    "# Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "m0KvUVwb8NjQ"
   },
   "outputs": [],
   "source": [
    "# class CMSELoss(nn.Module):\n",
    "#   \"\"\"\n",
    "#   Coordinate MSE loss\n",
    "#   input :\n",
    "#     y_pred = b, 21, 2   (coordinate of 21 keypoints)\n",
    "#     y_true = b, 21, 2   (keypoints, (y, x))\n",
    "#   \"\"\"\n",
    "#   def __init__(self):\n",
    "#     super().__init__()\n",
    "#     self.loss = nn.MSELoss()\n",
    "\n",
    "#   def forward(self, y_pred, y_true):\n",
    "#     y_true = torch.flip(y_true, [2]) # flip (y, x) to (x, y)\n",
    "#     return self.loss(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "AmZhlbbPhKJU"
   },
   "outputs": [],
   "source": [
    "# criterion = CMSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xGp8--mK9o8U"
   },
   "source": [
    "# Optimizer and Scheduler (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "rHIG6WpB9pyL"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# optimizer = torch.optim.Adadelta(model.parameters(), lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "0HGT7xWcO59p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000.0\n"
     ]
    }
   ],
   "source": [
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.002, steps_per_epoch=len(train_dl), epochs=config[\"n_epochs\"])\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, steps_per_epoch=len(train_dl), epochs=config[\"n_epochs\"], div_factor=2, final_div_factor=5, pct_start=0.09)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = config[\"pic_num\"] * (len(train_dl.dataset) / config[\"batch_size\"]) * config[\"n_epochs\"], eta_min = 0.0001)\n",
    "print((len(train_dl.dataset) / config[\"batch_size\"]) * config[\"n_epochs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvispd49hh0p"
   },
   "source": [
    "# Training\n",
    "  + Record the **`loss / epoch`** learning curve\n",
    "  + If using learning rate scheduler, record the **`lr / epoch`** curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40277/2063097751.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(weight_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_path = '/home/va8800/ken_ai/2024-ai-training-fianl-project/model_3.pth'\n",
    "# weight_path = '/home/STuser19/MID/model_2.pth'\n",
    "\n",
    "checkpoint = torch.load(weight_path)\n",
    "model.load_state_dict(checkpoint, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "CxgfsfnDeUft"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [01:42<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1: Learning Rate: 0.000999 Training Loss: 0.001932 Validation Loss:0.001916\n",
      "Validation loss decreased(inf-->0.001916). Saving model ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [01:38<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2: Learning Rate: 0.000996 Training Loss: 0.001944 Validation Loss:0.001908\n",
      "Validation loss decreased(0.001916-->0.001908). Saving model ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████▍                                 | 20/100 [00:23<01:33,  1.16s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 21\u001b[0m\n\u001b[1;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#######################\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# train the model #\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#######################\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#print(len(batch['image']))\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# clear the gradients of all optimized variables\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1327\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1293\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1293\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1294\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1295\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1131\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/multiprocessing/connection.py:440\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 440\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/multiprocessing/connection.py:1136\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1136\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            # initialize tracker for minimum validation loss\n",
    "# valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "valid_loss_min = np.inf # set initial \"min\" to infinity\n",
    "\n",
    "# initialize history for recording what we want to know\n",
    "history = []\n",
    "\n",
    "for epoch in range(config[\"n_epochs\"]):\n",
    "    # monitor training loss, validation loss and learning rate\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    lrs    = []\n",
    "    result = {'train_loss': [], 'val_loss': [], 'lrs': []}\n",
    "\n",
    "    # prepare model for training\n",
    "    model.train()\n",
    "\n",
    "    #######################\n",
    "    # train the model #\n",
    "    #######################\n",
    "    for batch in tqdm(train_dl):\n",
    "        #print(len(batch['image']))\n",
    "        for i in range(len(batch['image'])):\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            # print(batch['image'][i].shape)\n",
    "            output = model(batch['image'][i].to(config['device']))\n",
    "            # print(output.shape)\n",
    "            # print(batch['ans'][i].shape)\n",
    "            if(output.shape != batch['ans'][i].shape):\n",
    "                output = torch.nn.functional.interpolate(output, size=batch['ans'][i].shape[-2:], mode='bilinear', align_corners=False)\n",
    "            # calculate the loss\n",
    "            loss = ((output - batch['ans'][i].to(config['device'])) ** 2).mean()\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # record learning rate\n",
    "            lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            # update running training loss\n",
    "            train_loss += loss.item()*batch['image'][i].size(0)\n",
    "\n",
    "    ######################\n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for batch in val_dl:\n",
    "        # compute predicted outputs by passing inputs to the model\n",
    "        output = model(batch['image'][0].to(config['device']))\n",
    "        if(output.shape != batch['ans'][0].shape):\n",
    "                output = torch.nn.functional.interpolate(output, size=batch['ans'][i].shape[-2:], mode='bilinear', align_corners=False)\n",
    "        # calculate the loss\n",
    "        # loss = criterion(output, batch[''][0].to(config['device']))\n",
    "        loss = ((output - batch['ans'][0].to(config['device'])) ** 2).mean()\n",
    "\n",
    "        # update running validation loss\n",
    "        valid_loss += loss.item()*batch['image'][0].size(0)\n",
    "\n",
    "    # print training/validation statistics\n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/(len(train_dl.dataset)*config[\"pic_num\"])\n",
    "    result['train_loss'] = train_loss\n",
    "    valid_loss = valid_loss/len(val_dl.dataset)\n",
    "    result['val_loss'] = valid_loss\n",
    "    leaning_rate = lrs\n",
    "    result['lrs'] = leaning_rate\n",
    "    history.append(result)\n",
    "\n",
    "    print('Epoch {:2d}: Learning Rate: {:.6f} Training Loss: {:.6f} Validation Loss:{:.6f}'.format(\n",
    "        epoch+1,\n",
    "        leaning_rate[-1],\n",
    "        train_loss,\n",
    "        valid_loss\n",
    "        ))\n",
    "\n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print(\"Validation loss decreased({:.6f}-->{:.6f}). Saving model ..\".format(\n",
    "        valid_loss_min,\n",
    "        valid_loss\n",
    "        ))\n",
    "        torch.save(model.state_dict(),\"model_3_2.pth\")\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "em-PUm2HXEKg"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "torch.save(model.state_dict(), '/home/va8800/ken_ai/2024-ai-training-fianl-project/model_3_2.pth')\n",
    "# torch.save(model.state_dict(), '/home/STuser19/MID/v3/model.pth')\n",
    "print(\"Save model weight successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rVmvw-feZRM"
   },
   "outputs": [],
   "source": [
    "def plot_losses(history):\n",
    "  train_losses = [x.get('train_loss') for x in history]\n",
    "  val_losses = [x['val_loss'] for x in history]\n",
    "  plt.plot(train_losses, '-bx')\n",
    "  plt.plot(val_losses, '-rx')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.ylabel('loss')\n",
    "  plt.legend(['Training', 'Validation'])\n",
    "  plt.title('Loss vs. No. of epochs');\n",
    "\n",
    "plot_losses(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VXM16izaeZW5"
   },
   "outputs": [],
   "source": [
    "def plot_lrs(history):\n",
    "  lrs = np.concatenate([x.get('lrs', []) for x in history])\n",
    "  plt.plot(lrs)\n",
    "  plt.xlabel('Batch no.')\n",
    "  plt.ylabel('Learning rate')\n",
    "  plt.title('Learning Rate vs. Batch no.');\n",
    "\n",
    "plot_lrs(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5L12W9QQ-y1"
   },
   "source": [
    "# Load your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rQRCaHFQ_Bd"
   },
   "outputs": [],
   "source": [
    "weight_path = '/home/va8800/ken_ai/2024-ai-training-fianl-project/model_3_2.pth'\n",
    "# weight_path = '/home/STuser19/MID/v3/model_weights.pth'\n",
    "\n",
    "checkpoint = torch.load(weight_path)\n",
    "model.eval()\n",
    "model.load_state_dict(checkpoint, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LhRSBI9rsQzD"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "nKHgOCW7hAYS"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
