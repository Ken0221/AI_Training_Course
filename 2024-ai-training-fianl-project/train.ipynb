{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J93a2JElfqlj",
    "outputId": "73e52d48-6a39-436e-a3f7-b5d108b12027"
   },
   "outputs": [],
   "source": [
    "# !pip install thop\n",
    "# !pip install torchsummary\n",
    "# !pip install einops\n",
    "# !pip install -q kaggle\n",
    "# !pip install torch\n",
    "# !pip install numpy\n",
    "# !pip install opencv-python\n",
    "# !pip install matplotlib\n",
    "# !pip install natsort\n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "uJUnHoY3ag_1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from glob import glob\n",
    "from natsort import natsorted\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from thop import profile\n",
    "from thop import clever_format\n",
    "from torchsummary import summary\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.facecolor'] = '#ffffff'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: NVIDIA RTX A4000\n"
     ]
    }
   ],
   "source": [
    "result = torch.cuda.get_device_name(torch.cuda.current_device()) if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Current device: \" + result)\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDCdGBRHhNwR"
   },
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x4sCPVAkZjYp",
    "outputId": "62a1a022-ed50-4212-a3ae-cf4e9e8e1a77"
   },
   "outputs": [],
   "source": [
    "# !unzip DIV2K_train_HR.zip\n",
    "# !unzip DIV2K_valid_HR.zip\n",
    "# !unzip DIV2K_train_LR_bicubic_X3.zip\n",
    "# !unzip DIV2K_valid_LR_bicubic_X3.zip\n",
    "# !unzip DIV2k_train_LR_unknown_X3.zip\n",
    "# !unzip DIV2k_valid_LR_unknown_X3.zip\n",
    "\n",
    "# !unzip Set5.zip\n",
    "# !rm ./adarlab-ai-training.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gBEhrxWhRSR"
   },
   "source": [
    "# Checking Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "YRR5rwFsfwl4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/va8800/ken_ai/2024-ai-training-fianl-project\n",
      "3300\n",
      "250\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "\n",
    "img_train_data_dir_1 = './DIV2K_train_LR_bicubic/X3'\n",
    "img_train_data_dir_2 = './Flickr2K_LR_bicubic/train_X3'\n",
    "train_images = natsorted(glob(os.path.join(img_train_data_dir_1, '*.png'))) + natsorted(glob(os.path.join(img_train_data_dir_2, '*.png')))\n",
    "print(len(train_images))\n",
    "\n",
    "img_valid_data_dir_1 = './DIV2K_valid_LR_bicubic/X3'\n",
    "img_valid_data_dir_2 = './Flickr2K_LR_bicubic/val_X3'\n",
    "valid_images = natsorted(glob(os.path.join(img_valid_data_dir_1, '*.png'))) + natsorted(glob(os.path.join(img_valid_data_dir_2, '*.png')))\n",
    "print(len(valid_images))\n",
    "\n",
    "ans_train_data_dir_1 = './DIV2K_train_HR'\n",
    "ans_train_data_dir_2 = './Flickr2K_train_HR'\n",
    "train_ans = natsorted(glob(os.path.join(ans_train_data_dir_1, '*.png'))) + natsorted(glob(os.path.join(ans_train_data_dir_2, '*.png')))\n",
    "\n",
    "ans_valid_data_dir_1 = './DIV2K_valid_HR'\n",
    "ans_valid_data_dir_2 = './Flickr2K_val_HR'\n",
    "valid_ans = natsorted(glob(os.path.join(ans_valid_data_dir_1, '*.png'))) + natsorted(glob(os.path.join(ans_valid_data_dir_2, '*.png')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3462RLc7cxvd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9P5TGP47hDPR"
   },
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "9BDTfi-5fp5q"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"data_dir\": '/home/va8800/ken_ai/2024-ai-training-fianl-project',\n",
    "  \"data_num\": 900,\n",
    "  \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "  # you can set your own training configurations\n",
    "  \"batch_size\": 1,\n",
    "  \"learning_rate\": 0.0005,\n",
    "  \"n_epochs\": 50,\n",
    "  \"pic_num\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = None\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.RandomCrop(224),\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "6RRZpsGTfejS"
   },
   "outputs": [],
   "source": [
    "class TrainImg(Dataset):\n",
    "  def __init__(self, config, set_type=\"train\", transform=None):\n",
    "    self.device = config[\"device\"]\n",
    "    self.transform = transform\n",
    "    ###########################your code here#############################\n",
    "    # get the image path list -> self.image_names\n",
    "    self.train_image_names = train_images\n",
    "    self.train_ans_names = train_ans\n",
    "    self.valid_image_names = valid_images\n",
    "    self.valid_ans_names = valid_ans\n",
    "\n",
    "    ########################################################################\n",
    "    if set_type == \"train\":\n",
    "        # n_start = 0\n",
    "        # n_end = 4\n",
    "        self.image_names = self.train_image_names\n",
    "        self.ans_names = self.train_ans_names\n",
    "  \n",
    "    elif set_type == \"val\":\n",
    "        # n_start = 4\n",
    "        # n_end = config['data_num']\n",
    "        self.image_names = self.valid_image_names\n",
    "        self.ans_names = self.valid_ans_names\n",
    "\n",
    "    # self.image_names = self.image_names[n_start:n_end]\n",
    "    # self.ans_names = self.ans_names[n_start:n_end]\n",
    "  \n",
    "    ########################################################################\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.image_names)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    ########################################################################\n",
    "    # get the input image step by step\n",
    "    # 1. read the image using cv2\n",
    "    # 2. transpose the dimension from [h, w, 3] to [3, h, w]\n",
    "    # 3. from numpy array to tensor\n",
    "    # 4. normalize the value from [0, 255] to [0, 1]\n",
    "    image_name = self.image_names[idx]\n",
    "    image = cv2.imread(image_name, cv2.IMREAD_COLOR)\n",
    "    image = np.transpose(image, (2, 0, 1))  # transpose the dimension from [h, w, 3] to [3, h, w]\n",
    "    image = torch.from_numpy(image).float() # from numpy array to tensor\n",
    "    # image = image / 255 # normalize the value from [0, 255] to [0, 1]\n",
    "    \n",
    "    image_pic_arr = []\n",
    "    image_pic_arr.append(image)\n",
    "    length = config[\"pic_num\"] - 1\n",
    "    if self.transform:\n",
    "        for i in range(length):\n",
    "            image_pic = self.transform((image))\n",
    "            image_pic_arr.append(image_pic)\n",
    "    else:\n",
    "        for i in range(length):\n",
    "            image_pic_arr.append(image)\n",
    "    \n",
    "    image = image / 255 # normalize the value from [0, 255] to [0, 1]\n",
    "    \n",
    "    for i in range(length + 1):\n",
    "        image_pic_arr[i] = image_pic_arr[i] / 255\n",
    "        \n",
    "    ans_name = self.ans_names[idx]\n",
    "    ans = cv2.imread(ans_name, cv2.IMREAD_COLOR)\n",
    "    ans = np.transpose(ans, (2, 0, 1))  # transpose the dimension from [h, w, 3] to [3, h, w]\n",
    "    ans = torch.from_numpy(ans).float() # from numpy array to tensor\n",
    "    # image = image / 255 # normalize the value from [0, 255] to [0, 1]\n",
    "    \n",
    "    ans_pic_arr = []\n",
    "    ans_pic_arr.append(ans)\n",
    "    length = config[\"pic_num\"] - 1\n",
    "    if self.transform:\n",
    "        for i in range(length):\n",
    "            ans_pic = self.transform((ans))\n",
    "            ans_pic_arr.append(ans_pic)\n",
    "    else:\n",
    "        for i in range(length):\n",
    "            ans_pic_arr.append(ans)\n",
    "    \n",
    "    ans = ans / 255 # normalize the value from [0, 255] to [0, 1]\n",
    "    \n",
    "    for i in range(length + 1):\n",
    "        ans_pic_arr[i] = ans_pic_arr[i] / 255\n",
    "    \n",
    "    return {\n",
    "        'image_name': image_name,\n",
    "        'image': image_pic_arr,\n",
    "        'ans': ans_pic_arr,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "ncyC-Hrc8wx1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset length:  900\n",
      "Train dataset length:  3300\n",
      "Validation dataset length:  250\n"
     ]
    }
   ],
   "source": [
    "train_ds = TrainImg(config, set_type='train', transform=transform)\n",
    "val_ds = TrainImg(config, set_type='val')\n",
    "\n",
    "train_dl = DataLoader(train_ds, config[\"batch_size\"], shuffle=True, drop_last=True, num_workers=1)\n",
    "val_dl = DataLoader(val_ds, config[\"batch_size\"], shuffle=True, drop_last=True, num_workers=1)\n",
    "\n",
    "print(\"Total dataset length: \", config['data_num'])\n",
    "print(\"Train dataset length: \", len(train_ds))\n",
    "print(\"Validation dataset length: \", len(val_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKHgOCW7hAYS"
   },
   "source": [
    "# Show images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_img(image, vis=None, color_fixed=None, linewidth=1, img_order='rgb', draw_kp=True, kp_style=None):\n",
    "  \"\"\" Inpaints a hand stick figure into a matplotlib figure.\n",
    "  image:    original image input\n",
    "  coords_hw:  predicted keypoint (non normalized) -> [0, 224)\n",
    "  \"\"\"\n",
    "  if kp_style is None:\n",
    "    # kp_style[0] for circle radius, kp_style[1] for circle point thickness\n",
    "    kp_style = (1, 2)\n",
    "\n",
    "  # if image have four dimension like [1. 224. 224. 3] then squeeze to [3. 224. 3]\n",
    "  image = np.squeeze(image)\n",
    "\n",
    "  if len(image.shape) == 2:\n",
    "    image = np.expand_dims(image, 2)\n",
    "  s = image.shape\n",
    "  assert len(s) == 3, \"This only works for single images.\"\n",
    "\n",
    "  convert_to_uint8 = False\n",
    "\n",
    "  if s[2] == 1:\n",
    "    # grayscale case\n",
    "    image = (image - np.min(image)) / (np.max(image) - np.min(image) + 1e-4)\n",
    "    image = np.tile(image, [1, 1, 3])\n",
    "    pass\n",
    "\n",
    "  elif s[2] == 3:\n",
    "    # RGB case\n",
    "    if image.dtype == np.uint8:\n",
    "        convert_to_uint8 = True\n",
    "        image = image.astype('float64') / 255.0\n",
    "    elif image.dtype == np.float32:\n",
    "        # convert to gray image\n",
    "        image = np.mean(image, axis=2)\n",
    "        image = (image - np.min(image)) / (np.max(image) - np.min(image) + 1e-4)\n",
    "        image = np.expand_dims(image, 2)\n",
    "        image = np.tile(image, [1, 1, 3])\n",
    "  else:\n",
    "    assert 0, \"Unknown image dimensions.\"\n",
    "\n",
    "  colors = np.array(\n",
    "    [[0.4, 0.4, 0.4],\n",
    "    [0.4, 0.0, 0.0],\n",
    "    [0.6, 0.0, 0.0],\n",
    "    [0.8, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [0.4, 0.4, 0.0],\n",
    "    [0.6, 0.6, 0.0],\n",
    "    [0.8, 0.8, 0.0],\n",
    "    [1.0, 1.0, 0.0],\n",
    "    [0.0, 0.4, 0.2],\n",
    "    [0.0, 0.6, 0.3],\n",
    "    [0.0, 0.8, 0.4],\n",
    "    [0.0, 1.0, 0.5],\n",
    "    [0.0, 0.2, 0.4],\n",
    "    [0.0, 0.3, 0.6],\n",
    "    [0.0, 0.4, 0.8],\n",
    "    [0.0, 0.5, 1.0],\n",
    "    [0.4, 0.0, 0.4],\n",
    "    [0.6, 0.0, 0.6],\n",
    "    [0.7, 0.0, 0.8],\n",
    "    [1.0, 0.0, 1.0]]\n",
    "  )\n",
    "\n",
    "  if img_order == 'rgb':\n",
    "    # cv2 operation under BGR\n",
    "    colors = colors[:, ::-1]\n",
    "\n",
    "  color_map = {\n",
    "    'k': np.array([0.0, 0.0, 0.0]),\n",
    "    'w': np.array([1.0, 1.0, 1.0]),\n",
    "    'b': np.array([0.0, 0.0, 1.0]),\n",
    "    'g': np.array([0.0, 1.0, 0.0]),\n",
    "    'r': np.array([1.0, 0.0, 0.0]),\n",
    "    'm': np.array([1.0, 1.0, 0.0]),\n",
    "    'c': np.array([0.0, 1.0, 1.0])\n",
    "  }\n",
    "\n",
    "  if convert_to_uint8:\n",
    "    image = (image * 255).astype('uint8')\n",
    "\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_iter = iter(train_dl)\n",
    "# batch = next(batch_iter)\n",
    "\n",
    "# imgs = batch['image'] #return[img1, img2, img3]\n",
    "# print(batch['image'])\n",
    "# pic_num = 4\n",
    "# fig, axes = plt.subplots(len(imgs), pic_num, figsize=(15, 10))\n",
    "# for i in range(len(imgs)):\n",
    "#     for j in range(pic_num):\n",
    "#         # 將圖像和關鍵點轉換為 NumPy 格式\n",
    "#         img_np = imgs[i][j].permute(1, 2, 0).numpy()\n",
    "\n",
    "#         # 使用 draw_hand 函數繪製關鍵點\n",
    "#         trainimg = draw_img\n",
    "        \n",
    "#         # 顯示結果\n",
    "#         axes[i, j].imshow(trainimg)\n",
    "#         # axes[i, j].axis('off')\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eb_pWxc2xmOM"
   },
   "source": [
    "# Model\n",
    "\n",
    "+ Model Specifications:\n",
    "  + Input: **`[B, 3, 224, 224]`**\n",
    "  + Output: **`[B, 21, 2]`** --> 21 for the num of the landmarks, 2 for the coordinates in (x, y) format\n",
    "  + Layer: You can build up your own model architecture with no limitations.\n",
    "  + Cost: The computational cost (FLOPs) may not over **`20 GFLOPs`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "w9mqC8nZW7Tg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SuperResolution(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(64, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (PS): PixelShuffle(upscale_factor=3)\n",
      "  (relu): ReLU(inplace=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from model import SuperResolution\n",
    "\n",
    "Net = SuperResolution()\n",
    "print(Net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5cExNrO9Ad6"
   },
   "source": [
    "# Testing Model Computational Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "yTf0tYV2eFSh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================\n",
      "FLOPs : 7.096762368 G\n",
      "PARAMS : 0.054299 M \n",
      "============================\n",
      "Your FLOPs is smaller than 11.1 G.\n",
      "You will get this 15 points.\n",
      "Congratulations !!!\n"
     ]
    }
   ],
   "source": [
    "# # pseudo image\n",
    "# image = torch.rand(1, 3, 224, 224).cuda()\n",
    "\n",
    "# # define your model\n",
    "model = Net.to(config[\"device\"])\n",
    "\n",
    "# out = model(image)\n",
    "\n",
    "# # torchsummary report\n",
    "# summary(model, input_size=(3, 224, 224))\n",
    "# print(f'From input shape: {image.shape} to output shape: {out.shape}')\n",
    "\n",
    "# # thop report\n",
    "# macs, parm = profile(model, inputs=(image, ))\n",
    "# print(f'FLOPS: {macs * 2 / 1e9} G, Params: {parm / 1e6} M.')\n",
    "img = torch.randn(1, 3, 256, 256).to(config['device'])\n",
    "macs, params = profile(model, inputs=(img, ), verbose=False)\n",
    "flops = macs * 2 / 1e9  # G\n",
    "params = params / 1e6   # M\n",
    "print('============================')\n",
    "print(f'FLOPs : { flops } G')\n",
    "print(f'PARAMS : { params } M ')\n",
    "print('============================')\n",
    "if flops < 11.1:\n",
    "    print('Your FLOPs is smaller than 11.1 G.')\n",
    "    print('You will get this 15 points.')\n",
    "    print('Congratulations !!!')\n",
    "else:\n",
    "    print('Your FLOPs is larger than 11.1 G.')\n",
    "    print('You will not get this 10 points.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BB7B2iQI8IQb"
   },
   "source": [
    "# Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "m0KvUVwb8NjQ"
   },
   "outputs": [],
   "source": [
    "# class CMSELoss(nn.Module):\n",
    "#   \"\"\"\n",
    "#   Coordinate MSE loss\n",
    "#   input :\n",
    "#     y_pred = b, 21, 2   (coordinate of 21 keypoints)\n",
    "#     y_true = b, 21, 2   (keypoints, (y, x))\n",
    "#   \"\"\"\n",
    "#   def __init__(self):\n",
    "#     super().__init__()\n",
    "#     self.loss = nn.MSELoss()\n",
    "\n",
    "#   def forward(self, y_pred, y_true):\n",
    "#     y_true = torch.flip(y_true, [2]) # flip (y, x) to (x, y)\n",
    "#     return self.loss(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "AmZhlbbPhKJU"
   },
   "outputs": [],
   "source": [
    "# criterion = CMSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xGp8--mK9o8U"
   },
   "source": [
    "# Optimizer and Scheduler (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "rHIG6WpB9pyL"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "# optimizer = torch.optim.Adadelta(model.parameters(), lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "0HGT7xWcO59p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165000.0\n"
     ]
    }
   ],
   "source": [
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.002, steps_per_epoch=len(train_dl), epochs=config[\"n_epochs\"])\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, steps_per_epoch=len(train_dl), epochs=config[\"n_epochs\"], div_factor=2, final_div_factor=5, pct_start=0.09)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = config[\"pic_num\"] * (len(train_dl.dataset) / config[\"batch_size\"]) * config[\"n_epochs\"], eta_min = 0.00005)\n",
    "print((len(train_dl.dataset) / config[\"batch_size\"]) * config[\"n_epochs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvispd49hh0p"
   },
   "source": [
    "# Training\n",
    "  + Record the **`loss / epoch`** learning curve\n",
    "  + If using learning rate scheduler, record the **`lr / epoch`** curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_path = '/home/va8800/ken_ai/model_weights.pth'\n",
    "# weight_path = '/home/STuser19/MID/model_weights.pth'\n",
    "\n",
    "# checkpoint = torch.load(weight_path)\n",
    "# model.load_state_dict(checkpoint, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CxgfsfnDeUft"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████| 3300/3300 [09:17<00:00,  5.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1: Learning Rate: 0.000500 Training Loss: 0.003625 Validation Loss:0.002415\n",
      "Validation loss decreased(inf-->0.002415). Saving model ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████| 3300/3300 [08:49<00:00,  6.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2: Learning Rate: 0.000498 Training Loss: 0.002322 Validation Loss:0.002524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████| 3300/3300 [08:58<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3: Learning Rate: 0.000496 Training Loss: 0.002223 Validation Loss:0.002263\n",
      "Validation loss decreased(0.002415-->0.002263). Saving model ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████| 3300/3300 [09:02<00:00,  6.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4: Learning Rate: 0.000493 Training Loss: 0.002118 Validation Loss:0.002225\n",
      "Validation loss decreased(0.002263-->0.002225). Saving model ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████| 3300/3300 [09:01<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5: Learning Rate: 0.000489 Training Loss: 0.002058 Validation Loss:0.002187\n",
      "Validation loss decreased(0.002225-->0.002187). Saving model ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████| 3300/3300 [09:46<00:00,  5.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6: Learning Rate: 0.000484 Training Loss: 0.002029 Validation Loss:0.002113\n",
      "Validation loss decreased(0.002187-->0.002113). Saving model ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████| 3300/3300 [08:48<00:00,  6.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7: Learning Rate: 0.000479 Training Loss: 0.001998 Validation Loss:0.002113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████| 3300/3300 [09:13<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8: Learning Rate: 0.000472 Training Loss: 0.001994 Validation Loss:0.002542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████| 3300/3300 [09:44<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9: Learning Rate: 0.000465 Training Loss: 0.001965 Validation Loss:0.002080\n",
      "Validation loss decreased(0.002113-->0.002080). Saving model ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████| 3300/3300 [09:48<00:00,  5.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Learning Rate: 0.000457 Training Loss: 0.001961 Validation Loss:0.002083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████| 3300/3300 [09:51<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Learning Rate: 0.000448 Training Loss: 0.001937 Validation Loss:0.002076\n",
      "Validation loss decreased(0.002080-->0.002076). Saving model ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|███████████████████                           | 1367/3300 [04:09<06:15,  5.15it/s]"
     ]
    }
   ],
   "source": [
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            # initialize tracker for minimum validation loss\n",
    "# valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "valid_loss_min = np.inf # set initial \"min\" to infinity\n",
    "\n",
    "# initialize history for recording what we want to know\n",
    "history = []\n",
    "\n",
    "for epoch in range(config[\"n_epochs\"]):\n",
    "    # monitor training loss, validation loss and learning rate\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    lrs    = []\n",
    "    result = {'train_loss': [], 'val_loss': [], 'lrs': []}\n",
    "\n",
    "    # prepare model for training\n",
    "    model.train()\n",
    "\n",
    "    #######################\n",
    "    # train the model #\n",
    "    #######################\n",
    "    for batch in tqdm(train_dl):\n",
    "        for i in range(len(batch['image'])):\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            # print(batch['image'][i].shape)\n",
    "            output = model(batch['image'][i].to(config['device']))\n",
    "            # print(output.shape)\n",
    "            # print(batch['ans'][i].shape)\n",
    "            if(output.shape != batch['ans'][i].shape):\n",
    "                output = torch.nn.functional.interpolate(output, size=batch['ans'][i].shape[-2:], mode='bilinear', align_corners=False)\n",
    "            # calculate the loss\n",
    "            loss = ((output - batch['ans'][i].to(config['device'])) ** 2).mean()\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # record learning rate\n",
    "            lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            # update running training loss\n",
    "            train_loss += loss.item()*batch['image'][i].size(0)\n",
    "\n",
    "    ######################\n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for batch in val_dl:\n",
    "        # compute predicted outputs by passing inputs to the model\n",
    "        output = model(batch['image'][0].to(config['device']))\n",
    "        if(output.shape != batch['ans'][i].shape):\n",
    "                output = torch.nn.functional.interpolate(output, size=batch['ans'][i].shape[-2:], mode='bilinear', align_corners=False)\n",
    "        # calculate the loss\n",
    "        # loss = criterion(output, batch[''][0].to(config['device']))\n",
    "        loss = ((output - batch['ans'][i].to(config['device'])) ** 2).mean()\n",
    "\n",
    "        # update running validation loss\n",
    "        valid_loss += loss.item()*batch['image'][0].size(0)\n",
    "\n",
    "    # print training/validation statistics\n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/(len(train_dl.dataset)*config[\"pic_num\"])\n",
    "    result['train_loss'] = train_loss\n",
    "    valid_loss = valid_loss/len(val_dl.dataset)\n",
    "    result['val_loss'] = valid_loss\n",
    "    leaning_rate = lrs\n",
    "    result['lrs'] = leaning_rate\n",
    "    history.append(result)\n",
    "\n",
    "    print('Epoch {:2d}: Learning Rate: {:.6f} Training Loss: {:.6f} Validation Loss:{:.6f}'.format(\n",
    "        epoch+1,\n",
    "        leaning_rate[-1],\n",
    "        train_loss,\n",
    "        valid_loss\n",
    "        ))\n",
    "\n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print(\"Validation loss decreased({:.6f}-->{:.6f}). Saving model ..\".format(\n",
    "        valid_loss_min,\n",
    "        valid_loss\n",
    "        ))\n",
    "        torch.save(model.state_dict(),\"model.pt\")\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "em-PUm2HXEKg"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "torch.save(model.state_dict(), '/home/va8800/ken_ai/2024-ai-training-fianl-project/model.pth')\n",
    "# torch.save(model.state_dict(), '/home/STuser19/MID/v3/model.pth')\n",
    "print(\"Save model weight successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rVmvw-feZRM"
   },
   "outputs": [],
   "source": [
    "def plot_losses(history):\n",
    "  train_losses = [x.get('train_loss') for x in history]\n",
    "  val_losses = [x['val_loss'] for x in history]\n",
    "  plt.plot(train_losses, '-bx')\n",
    "  plt.plot(val_losses, '-rx')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.ylabel('loss')\n",
    "  plt.legend(['Training', 'Validation'])\n",
    "  plt.title('Loss vs. No. of epochs');\n",
    "\n",
    "plot_losses(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VXM16izaeZW5"
   },
   "outputs": [],
   "source": [
    "def plot_lrs(history):\n",
    "  lrs = np.concatenate([x.get('lrs', []) for x in history])\n",
    "  plt.plot(lrs)\n",
    "  plt.xlabel('Batch no.')\n",
    "  plt.ylabel('Learning rate')\n",
    "  plt.title('Learning Rate vs. Batch no.');\n",
    "\n",
    "plot_lrs(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5L12W9QQ-y1"
   },
   "source": [
    "# Load your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rQRCaHFQ_Bd"
   },
   "outputs": [],
   "source": [
    "weight_path = '/home/va8800/ken_ai/2024-ai-training-fianl-project/model.pth'\n",
    "# weight_path = '/home/STuser19/MID/v3/model_weights.pth'\n",
    "\n",
    "checkpoint = torch.load(weight_path)\n",
    "model.eval()\n",
    "model.load_state_dict(checkpoint, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LhRSBI9rsQzD"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "nKHgOCW7hAYS"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
